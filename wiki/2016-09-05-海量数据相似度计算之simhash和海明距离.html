<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://bruceasu.github.io/rss.xml"
      title="RSS feed for https://bruceasu.github.io/"/>
<title>海量数据相似度计算之SIMHASH和海明距离</title>
<meta name="author" content="Bruce">
<meta name="referrer" content="no-referrer">
<link href= "/styles/org-manual.css" rel="stylesheet" type="text/css" />
<link rel="icon" href="static/favicon.ico">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" href="/styles/font.css">
<link rel="stylesheet" media="screen and (min-width: 600px)" href="/styles/post.css">
<link rel="stylesheet" media="screen and (max-width: 600px)" href="/styles/post_mobile.css">
<link rel="stylesheet" media="screen and (min-width: 600px)" href="/styles/navigatebar.css">
<link rel="stylesheet" media="screen and (max-width: 600px)" href="/styles/navigatebar_mobile.css">
<link rel="stylesheet" href="/theme/highlight.css">

</head>
<body>
<div class="navigatebar">
    <div class="navigatebar-button navigatebar-mine">
        <a href="/index.html">Free World</a>
    </div>
    <div class="navigatebar-slogan">
        「生活可以更简单, 欢迎来到我的开源世界」
    </div>
    <div class="navigatebar-button">
        <a href="/index.html">Home</a>
    </div>
    <div class="navigatebar-button">
        <a href="/tags.html">Tags</a>
    </div>
    <div class="navigatebar-button">
        <a href="/rss.xml">Feeds</a>
    </div>
    <div class="navigatebar-button navigatebar-about">
        <a href="/about.html">About</a>
    </div>
</div>

      <div class="content-area">
<div class="title">海量数据相似度计算之SIMHASH和海明距离</div>
<div class="category-area"><a href="https://bruceasu.github.io/tags.html#reprint"><div class="category">reprint</div></a> </div>
<div class="char-counter">2016-09-05</div>
        <div id="content">
<p>
海量数据相似度计算之simhash和海明距离
</p>

<p>
Posted on 25 八月, 2013  by  lanceyan
</p>


<p>
通过采集系统我们采集了大量文本数据，但是文本中有很多重复数据影响我们对于结果的分
析。分析前我们需要对这些数据去除重复，如何选择和设计文本的去重算法？常见的有 余
弦夹角算法、欧式距离、Jaccard相似度、最长公共子串、编辑距离等。这些算法对于待比
较的文本数据不多时还比较好用，如果我们的爬虫每天采集的数 据以千万计算，我们如何
对于这些海量千万级的数据进行高效的合并去重。最简单的做法是拿着待比较的文本和数据
库中所有的文本比较一遍如果是重复的数据就标 示为重复。看起来很简单，我们来做个测
试，就拿最简单的两个数据使用Apache提供的 Levenshtein for 循环100w次计算这两个数
据的相似度。代码结果如下：
</p>

<div class="org-src-container">
<pre class="src src-java"><span style="font-weight: bold; text-decoration: underline;">String</span> <span style="font-weight: bold; font-style: italic;">s1</span> = <span style="font-style: italic;">"&#20320;&#22920;&#22920;&#21898;&#20320;&#22238;&#23478;&#21507;&#39277;&#21734;&#65292;&#22238;&#23478;&#32599;&#22238;&#23478;&#32599;"</span> ;
<span style="font-weight: bold; text-decoration: underline;">String</span> <span style="font-weight: bold; font-style: italic;">s2</span> = <span style="font-style: italic;">"&#20320;&#22920;&#22920;&#21483;&#20320;&#22238;&#23478;&#21507;&#39277;&#21862;&#65292;&#22238;&#23478;&#32599;&#22238;&#23478;&#32599;"</span> ;

<span style="font-weight: bold; text-decoration: underline;">long</span> <span style="font-weight: bold; font-style: italic;">t1</span> = System.currentTimeMillis();

<span style="font-weight: bold;">for</span> (<span style="font-weight: bold; text-decoration: underline;">int</span> <span style="font-weight: bold; font-style: italic;">i</span> = 0; i &lt; 1000000; i++) {
    <span style="font-weight: bold; text-decoration: underline;">int</span> <span style="font-weight: bold; font-style: italic;">dis</span> = StringUtils .getLevenshteinDistance(s1, s2);
}

<span style="font-weight: bold; text-decoration: underline;">long</span> <span style="font-weight: bold; font-style: italic;">t2</span> = System.currentTimeMillis();

System. out .println(<span style="font-style: italic;">" &#32791;&#36153;&#26102;&#38388;&#65306; "</span> + (t2 - t1) + <span style="font-style: italic;">" &#160;ms "</span>);
</pre>
</div>
<pre class="example">
耗费时间： 4266  ms

</pre>

<p>
大跌眼镜，居然计算耗费4秒。假设我们一天需要比较100w次，光是比较100w次的数据是否
重复就需要4s，就算4s一个文档，单线程一分钟才处 理15个文档，一个小时才900个，一天
也才21600个文档，这个数字和一天100w相差甚远，需要多少机器和资源才能解决。
</p>

<p>
为此我们需要一种应对于海量数据场景的去重方案，经过研究发现有种叫 local sensitive
hash 局部敏感哈希 的东西，据说这玩意可以把文档降维到hash数字，数字两两计算运算量
要小很多。查找很多文档后看到google对于网页去重使用的是simhash，他 们每天需要处理
的文档在亿级别，大大超过了我们现在文档的水平。既然老大哥也有类似的应用，我们也赶
紧尝试下。simhash是由 Charikar 在2002年提出来的，参考 《Similarity estimation
techniques from rounding algorithms》 。 介绍下这个算法主要原理，为了便于理解尽
量不使用数学公式，分为这几步：
</p>

<ol class="org-ol">
<li>分词，把需要判断文本分词形成这个文章的特征单词。最后形成去掉噪音词的单词序列
并为每个词加上权重，我们 假设权重分为5个级别（1~5）。比如：“ 美国“51区”雇员称
内部有9架飞碟，曾看见灰色外星人 ” ==&gt; 分词后为 “ 美国（4） 51区（5） 雇员（3）
称（1） 内部（2） 有（1） 9架（3） 飞碟（5） 曾（1） 看见（3） 灰色（4） 外星
人（5）”，括号里是代表单词在整个句子里重要程度，数字越大越重要。</li>

<li>hash，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51
区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字，还记得文章
开头说过的吗，要把文章变为数字计算才能提高相似度计算性能，现在是降维过程进行
时。</li>

<li>加权，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，比如“美国”
的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”；“51区”的hash值为“101011”，
通过加权计算为 “ 5 -5 5 -5 5 5”。</li>

<li>合并，把上面各个单词算出来的序列值累加，变成只有一个序列串。比如 “美国”的 “4
-4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”， 把每一位进行累加， “4+5 -4+-5 -4+5
4+-5 -4+5 4+5” ==》 “9 -9 1 -1 1 9”。这里作为示例只算了两个单词的，真实计算需
要把所有单词的序列串累加。</li>

<li>降维，把4步算出来的 “9 -9 1 -1 1 9” 变成 0 1 串，形成我们最终的simhash签名。
如果每一位大于0 记为 1，小于0 记为 0。最后算出结果为：“1 0 1 0 1 1”。</li>
</ol>

<p>
整个过程图为：
</p>

<p>
大家可能会有疑问，经过这么多步骤搞这么麻烦，不就是为了得到个 0 1 字符串吗？我直
接把这个文本作为字符串输入，用hash函数生成 0 1 值更简单。其实不是这样的，传统
hash函数解决的是生成唯一值，比如 md5、hashmap等。md5是用于生成唯一签名串，只要稍
微多加一个字符md5的两个数字看起来相差甚远；hashmap也是用于键值对查找，便于 快速
插入和查找的数据结构。不过我们主要解决的是文本相似度计算，要比较的是两个文章是否
相识，当然我们降维生成了hashcode也是用于这个目的。看 到这里估计大家就明白了，我
们使用的simhash就算把文章中的字符串变成 01 串也还是可以用于计算相似度的，而传统
的hashcode却不行。我们可以来做个测试，两个相差只有一个字符的文本串，“你妈妈喊你
回家吃饭哦，回家罗回家罗” 和 “你妈妈叫你回家吃饭啦，回家罗回家罗”。
</p>

<p>
通过simhash计算结果为：
</p>

<pre class="example">
1000010010101101111111100000101011010001001111100001001011001011

</pre>

<pre class="example">
1000010010101101011111100000101011010001001111100001101010001011

</pre>

<p>
通过 hashcode计算为：
</p>

<pre class="example">
1111111111111111111111111111111110001000001100110100111011011110

</pre>

<pre class="example">
1010010001111111110010110011101

</pre>

<p>
大家可以看得出来，相似的文本只有部分 01 串变化了，而普通的hashcode却不能做到，这
个就是局部敏感哈希的魅力。目前Broder提出的shingling算法和Charikar的 simhash算法
应该算是业界公认比较好的算法。在simhash的发明人Charikar的论文中并没有给出具体的
simhash算法和证明，“量子图灵”得出的证明simhash是由随机超平面hash算法演变而来的。
现在通过这样的转换，我们把库里的文本都转换为simhash 代码，并转换为long类型存储，
空间大大减少。现在我们虽然解决了空间，但是如何计算两个simhash的相似度呢？难道是
比较两个simhash的 01有多少个不同吗？对的，其实也就是这样，我们通过海明距离
（Hamming distance）就可以计算出两个simhash到底相似不相似。两个simhash对应二进制
（01串）取值不同的数量称为这两个simhash的海 明距离。举例如下： 10101 和 00110 从
第一位开始依次有第一位、第四、第五位不同，则海明距离为3。对于二进制字符串的a和b，
海明距离为等于在a XOR b运算结果中1的个数（普遍算法）。为了高效比较，我们预先加载
了库里存在文本并转换为simhash code 存储在内存空间。来一条文本先转换为 simhash
code，然后和内存里的simhash code 进行比较，测试100w次计算在100ms。速度大大提升。
</p>

<p>
未完待续：
</p>

<ol class="org-ol">
<li>目前速度提升了但是数据是不断增量的，如果未来数据发展到一个小时100w，按现在一
次100ms，一个线程处理一秒钟 10次，一分钟 60 * 10 次，一个小时 60*10 *60 次 =
36000次，一天 60*10*60*24 = 864000次。 我们目标是一天100w次，通过增加两个线程
就可以完成。但是如果要一个小时100w次呢？则需要增加30个线程和相应的硬件资源保
证速度能够达到，这样 成本也上去了。能否有更好的办法，提高我们比较的效率？</li>
<li>通过大量测试，simhash用于比较大文本，比如500字以上效果都还蛮好，距离小于3的基
本都是相似，误判率也比较低。但是如果我们处理的 是微博信息，最多也就140个字，
使用simhash的效果并不那么理想。看如下图，在距离为3时是一个比较折中的点，在距
离为10时效果已经很差了，不 过我们测试短文本很多看起来相似的距离确实为10。如果
使用距离为3，短文本大量重复信息不会被过滤，如果使用距离为10，长文本的错误率也
非常高，如何 解决？</li>
</ol>

<p>
参考：
</p>

<p>
Detecting near-duplicates for web crawling.
</p>

<p>
Similarity estimation techniques from rounding algorithms.
</p>
<ul class="org-ul">
<li><a href="http://en.wikipedia.org/wiki/Locality_sensitive_hashing">http://en.wikipedia.org/wiki/Locality_sensitive_hashing</a></li>
<li><a href="http://en.wikipedia.org/wiki/Hamming_distance">http://en.wikipedia.org/wiki/Hamming_distance</a></li>
</ul>

<p>
simHash 简介以及 java 实现
</p>

<p>
simhash原理推导
</p>

        </div>    </div>
    <div id="postamble" class="status"><div id="archive" style="padding-top: 3em; padding-bottom: 2em;"><a href="/archive.html">其它文章</a></div><script src="/js/av-min-1.5.0.js"></script>    </div>
</body>
</html>
